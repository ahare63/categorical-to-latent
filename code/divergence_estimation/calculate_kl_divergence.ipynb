{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"build_KL_samples.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"3_AdepEnqaiT","executionInfo":{"status":"ok","timestamp":1606762786958,"user_tz":300,"elapsed":7039,"user":{"displayName":"Adam Hare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixZU71qfDiWZdtT_DDCqQJkmHnTS8Yc9_AiWSsrQ=s64","userId":"17238913879148436522"}}},"source":["from collections import Counter\n","import embeddings\n","import faiss\n","import json\n","import glob\n","import numpy as np\n","import random\n","\n","from kl_divergence import get_article_dicts, get_kl_divs, get_word_2_ind, B\n","from process_words import *\n","from process_sentences import *\n","from similarity_search import *\n","import utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AYs4lPaEEl0","executionInfo":{"status":"ok","timestamp":1606762793825,"user_tz":300,"elapsed":277,"user":{"displayName":"Adam Hare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixZU71qfDiWZdtT_DDCqQJkmHnTS8Yc9_AiWSsrQ=s64","userId":"17238913879148436522"}}},"source":["# Sample num_samples (non-unique) words from the word_2_count dict and return\n","def get_sample(word_2_count, num_samples):\n","  word_array = []\n","  for k, v in word_2_count.items():\n","    word_array += [k]*v\n","  random.shuffle(word_array)\n","  return word_array[:num_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4NKopVzq2Qt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606763482270,"user_tz":300,"elapsed":687044,"user":{"displayName":"Adam Hare","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixZU71qfDiWZdtT_DDCqQJkmHnTS8Yc9_AiWSsrQ=s64","userId":"17238913879148436522"}},"outputId":"1d8d2d66-c9be-4124-edca-e2d3fd824338"},"source":["# For a given source_directory, for each file in the directory sample num_samples words and write results to output_directory\n","# output_directory is assumed to exist\n","def sample_from_text(num_samples, source_dir, out_dir):\n","  embedder = embeddings.FastTextEmbedding()\n","  stopwords = utils.make_stopwords_list() + [\"'\", \"’\", \"”\", \"(\", \")\", \"‘\"]\n","  locs = ['middle', 'high', 'college']\n","  for loc in locs:\n","    files = glob.glob(f'{source_dir}/*')\n","    for path in files:\n","      # Get the name of the book from the filename\n","      book = path.split(\"/\")[-1]\n","      print(book)\n","      # Read all words from the book\n","      _, _, word_2_count, _ = get_words(path, embedder, keep_misses=False, stopwords=stopwords)\n","      # If the book is too short, ignore it\n","      if sum(word_2_count.values()) < num_samples:\n","        print(\"MISS\", book, sum(word_2_count.values()))\n","        continue\n","      # Otherwise, sample and write to output dir\n","      else:\n","        text = \" \".join(get_sample(word_2_count, num_samples))\n","        with open(f'{out_dir}/{book}', 'w') as out_file:\n","          out_file.write(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reads data from dataset, calculates pairwise KL divergences, and writes the results according to the get_kl_divs_function\n","def calculate_pairwise_divergences(dataset):\n","    # dataset could be something like \"books_sample_40k\"\n","    ind_2_auth, ind_2_text, ind_2_counts, ind_2_embs, ind_2_p = get_article_dicts(dataset)\n","    ep = 10E-5\n","    a = [1-ep, 1+ep]\n","    _ = get_kl_divs(dataset, ind_2_p, ind_2_auth, ind_2_embs, ind_2_counts, alphas=a, gpu=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Build from individual files to single json\n","# out_file should be a json\n","def combine_results(source_dir, out_file):\n","  kl_og = {}\n","  kl_3 = {}\n","  kl_5 = {}\n","  kl_10 = {}\n","  kl_25 = {}\n","  kl_50 = {}\n","  kl_100 = {}\n","\n","  for f in glob.glob(f\"{source_dir}/*.json\"):\n","    with open(f, 'r') as out_file:\n","      data = json.load(out_file)\n","      left = f.split(\"/\")[-1].split(\".\")[0]\n","      for right in data.keys():\n","        kl_og[(left, right_fixed)] = data[right][\"original\"]\n","        kl_3[(left, right_fixed)] = data[right][\"3\"]\n","        kl_5[(left, right_fixed)] = data[right][\"5\"]\n","        kl_10[(left, right_fixed)] = data[right][\"10\"]\n","        kl_25[(left, right_fixed)] = data[right][\"25\"]\n","        kl_50[(left, right_fixed)] = data[right][\"50\"]\n","        kl_100[(left, right_fixed)] = data[right][\"100\"]\n","\n","  data = {}\n","  data[\"pairs\"] = list(kl_og.keys())\n","  data[\"original\"] = list(kl_og.values())\n","  data[\"3\"] = list(kl_3.values())\n","  data[\"5\"] = list(kl_5.values())\n","  data[\"10\"] = list(kl_10.values())\n","  data[\"25\"] = list(kl_25.values())\n","  data[\"50\"] = list(kl_50.values())\n","  data[\"100\"] = list(kl_100.values())\n","\n","  with open(out_file, 'w') as out_file:\n","    json.dump(data, out_file, indent=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# For a given file, \n","def get_predictions(source_file, out_file, works):\n","  # Get the \"true\" label for each book\n","  with open('authorship_categories.json', 'r') as f:\n","    categories = json.load(f)\n","\n","  work_2_auth = {}\n","  for auth in categories[\"author\"].keys():\n","    for work in categories[\"author\"][auth]:\n","      work_2_auth[work] = auth\n","\n","  work_2_level = {}\n","  for level in categories[\"reading_level\"].keys():\n","    for work in categories[\"reading_level\"][level]:\n","      work_2_level[work] = level\n","\n","  work_2_genre = {}\n","  for genre in categories[\"genre\"].keys():\n","    for work in categories[\"genre\"][genre]:\n","      work_2_genre[work] = genre\n","\n","  # Load results from this file and initialize data structure\n","  with open(source_file, 'r') as f:\n","    data = json.load(f)\n","  left = file_name.split(\"/\")[-1].split(\".\")[0]\n","  new = {}\n","  new[\"author\"] = {k: {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []} for k in categories[\"author\"].keys()}\n","  new[\"level\"] = {k: {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []} for k in categories[\"reading_level\"].keys()}\n","  new[\"genre\"] = {k: {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []} for k in categories[\"genre\"].keys()}\n","\n","  # Gather the results for every other work\n","  for right in data.keys():\n","    if right in [\"author\", \"level\", \"genre\"]:\n","      continue\n","    if right not in works:\n","      continue\n","    for val in [\"original\", \"3\", \"5\", \"10\", \"25\", \"50\", \"100\"]:\n","      new[\"author\"][work_2_auth[right]][val].append(data[right][val])\n","      new[\"level\"][work_2_level[right]][val].append(data[right][val])\n","      new[\"genre\"][work_2_genre[right]][val].append(data[right][val])\n","\n","  # Take the average\n","  for comp in [\"author\", \"level\", \"genre\"]:\n","    for cat in new[comp].keys():\n","      for val in [\"original\", \"3\", \"5\", \"10\", \"25\", \"50\", \"100\"]:\n","        new[comp][cat][f\"{val}_avg\"] = np.mean(new[comp][cat][val])\n","  \n","  # Get the number of times each label appears\n","  auth = []\n","  level = []\n","  genre = []\n","  for w in works:\n","    auth.append(work_2_auth[w])\n","    level.append(work_2_level[w])\n","    genre.append(work_2_genre[w])\n","\n","  counts = {\"author\": Counter(auth), \"level\": Counter(level), \"genre\": Counter(genre)}\n","\n","  final = {}\n","  final[\"predictions\"] = {\"author\": {\"original\": \"\", \"3\": \"\", \"5\": \"\", \"10\": \"\", \"25\": \"\", \"50\": \"\", \"100\": \"\", \"nearest\": {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []}},\n","                         \"level\": {\"original\": \"\", \"3\": \"\", \"5\": \"\", \"10\": \"\", \"25\": \"\", \"50\": \"\", \"100\": \"\", \"nearest\": {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []}},\n","                         \"genre\": {\"original\": \"\", \"3\": \"\", \"5\": \"\", \"10\": \"\", \"25\": \"\", \"50\": \"\", \"100\": \"\", \"nearest\": {\"original\": [], \"3\": [], \"5\": [], \"10\": [], \"25\": [], \"50\": [], \"100\": []}}}\n","  for comp in [\"author\", \"level\", \"genre\"]:\n","    for val in [\"original\", \"3\", \"5\", \"10\", \"25\", \"50\", \"100\"]:\n","      min_val = 1000\n","      min_category = \"\"\n","      for cat in new[comp].keys():\n","        # This is the new label if 1) the divergence is less than the previously recorded divergence\n","        # and 2) the label applies to at least two works in the dataset\n","        if new[comp][cat][f\"{val}_avg\"] < min_val and counts[comp][cat] > 1:\n","          min_val = new[comp][cat][f\"{val}_avg\"]\n","          min_category = cat\n","      final[\"predictions\"][comp][val] = min_category\n","      # Order the rest of the results by how far off from optimal they are. This helps with debugging\n","      nearest_list = []\n","      for cat in new[comp].keys():\n","        if cat != min_category and not np.isnan(new[comp][cat][f\"{val}_avg\"]):\n","          if counts[comp][cat] > 1:\n","            nearest_list.append((cat, new[comp][cat][f\"{val}_avg\"] - min_val))\n","          else:\n","            nearest_list.append((cat, -1))\n","\n","      nearest_list.sort(key=lambda x: x[1])\n","      final[\"predictions\"][comp][\"nearest\"][val] = nearest_list\n","\n","  # Dump results to file\n","  with open(out_file, \"w\") as f:\n","    json.dump(final, f, indent=2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get accuracy stats\n","def show_accuracy(source_dir):\n","    # Load \"true\" results\n","    with open('authorship_categories.json', 'r') as f:\n","    categories = json.load(f)\n","\n","    work_2_auth = {}\n","    for auth in categories[\"author\"].keys():\n","    for work in categories[\"author\"][auth]:\n","        work_2_auth[work] = auth\n","\n","    work_2_level = {}\n","    for level in categories[\"reading_level\"].keys():\n","    for work in categories[\"reading_level\"][level]:\n","        work_2_level[work] = level\n","\n","    work_2_genre = {}\n","    for genre in categories[\"genre\"].keys():\n","    for work in categories[\"genre\"][genre]:\n","        work_2_genre[work] = genre\n","\n","\n","    scores = {}\n","    for comp in [\"author\", \"level\", \"genre\"]:\n","    scores[comp] = {}\n","    for val in [\"original\", \"3\", \"5\", \"10\", \"25\", \"50\", \"100\"]:\n","        scores[comp][val] = 0\n","\n","    # Collect list of books\n","    files = glob.glob(f\"{source_dir}/*.json\")\n","    works =[x.split(\"/\")[-1].split(\".\")[0] for x in files]\n","\n","\n","    number_files = len(files)\n","    for f in files:\n","        with open(f, 'r') as in_file:\n","            data = json.load(in_file)[\"predictions\"]\n","        book = f.split(\"/\")[-1].split(\".\")[0]\n","        for val in [\"original\", \"3\", \"5\", \"10\", \"25\", \"50\", \"100\"]:\n","            if data[\"author\"][val] == work_2_auth[book]:\n","                scores[\"author\"][val] += 1\n","            if data[\"level\"][val] == work_2_level[book]:\n","            scores[\"level\"][val] += 1\n","            if data[\"genre\"][val] == work_2_genre[book]:\n","            scores[\"genre\"][val] += 1\n","\n","    print(scores)\n","    print(number_files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# An example usage\n","sample_from_text(32000, './data/books', './data/books_sample_32k')\n","calculate_pairwise_divergences('books_sample_32k')\n","combine_results(\"./kl_results/books_sample_32k\", \"./kl_combined_32k.json\")\n","\n","files = glob.glob(\"./kl_results/books_sample_32k/*.json\")\n","works = [x.split(\"/\")[-1].split(\".\")[0] for x in files]\n","for f in files:\n","  book = f.split(\"/\")[-1].split(\".\")[0]\n","  get_avgs(f, f\"./kl_predictions_32k/{book}.json\", sample)\n","\n","show_accuracy(\"32k\")"]}]}